{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbuvQeHIaecX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import matplotlib as plt\n",
    "import folium\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except BaseException:\n",
    "    import pickle\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "#https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbuvQeHIaecX"
   },
   "outputs": [],
   "source": [
    "def timer(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time()\n",
    "        print('Elapsed time: {}'.format(end-start))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "@timer\n",
    "def get_date(base_dir):\n",
    "    new_time = []\n",
    "    for file in listdir(base_dir):\n",
    "        file_path = f'{base_dir}/{file}'\n",
    "        match=file.split(\"_\")[1]\n",
    "        date = pd.to_datetime(match, format = \"%Y%m%d%H\").strftime('%d/%m/%Y')\n",
    "        time = (datetime.strptime(match, \"%Y%m%d%H\") + timedelta(hours=6)).strftime('%H:%M')\n",
    "        new_time.append(date + \" \" + time)\n",
    "    return new_time\n",
    "\n",
    "\n",
    "@timer\n",
    "def get_variables(base_dir, var_list, diccionario, nz=26):\n",
    "    d3_var = [\"HGTprs\", \"CLWMRprs\", \"RHprs\",\"Velprs\",\"UGRDprs\",\"VGRDprs\",\"TMPprs\"]\n",
    "    d2_var = [\"HGTsfc\", \"MSLETmsl\", \"PWATclm\", \"RH2m\", \"Vel100m\", \"UGRD100m\", \"VGRD100m\",\n",
    "            \"Vel80m\", \"UGRD80m\", \"VGRD80m\", \"Vel10m\", \"UGRD10m\", \"VGRD10m\", \"GUSTsfc\",\n",
    "            \"TMPsfc\", \"TMP2m\", \"no4LFTXsfc\", \"CAPEsfc\", \"SPFH2m\", \"SPFH80m\"]\n",
    "\n",
    "    lst = []\n",
    "  \n",
    "    for file in listdir(base_dir):\n",
    "        file_path = f'{base_dir}/{file}'\n",
    "        e_file = []\n",
    "        for key, value in diccionario.items():\n",
    "\n",
    "            if key in set(var_list).intersection(d3_var): #d3_var:\n",
    "                corte = value[0] + int(((value[1])/26)*nz)\n",
    "                e_file.append(np.fromfile(file_path, dtype=np.float32)[value[0]:corte])\n",
    "\n",
    "            elif key in set(var_list).intersection(d2_var):#d2_var:\n",
    "                e_file.append(np.fromfile(file_path, dtype=np.float32)[value[0]:value[1]])\n",
    "        lst.append(e_file)\n",
    "  \n",
    "    return lst\n",
    "\n",
    "@timer\n",
    "def setup_x(dataframe):\n",
    "    \"\"\"Flat variables values for model training\"\"\"\n",
    "    dataframe.reset_index(level=0, inplace=True)\n",
    "    row_list =[] \n",
    "    for index, rows in dataframe.iterrows(): \n",
    "        my_list = [rows.RHprs, rows.Velprs, rows.TMPprs, rows.Vel100m, rows.Vel80m,rows.TMPsfc, rows.SPFH80m]\n",
    "        row_list.append(my_list) \n",
    "\n",
    "    a = [np.concatenate(row_list[i]) for i in range(len(row_list))]\n",
    "    train_ = pd.DataFrame(a, index=dataframe[\"index\"])\n",
    "    return train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGakaqyhTMiK"
   },
   "outputs": [],
   "source": [
    "# Bayesian optimization\n",
    "@timer\n",
    "class HPOpt(object):\n",
    "\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "\n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "\n",
    "    def xgb_reg(self, para):\n",
    "        reg = xgb.XGBRegressor(**para['reg_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    def lgb_reg(self, para):\n",
    "        reg = lgb.LGBMRegressor(**para['reg_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    def ctb_reg(self, para):\n",
    "        reg = ctb.CatBoostRegressor(**para['reg_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    def train_reg(self, reg, para):\n",
    "        reg.fit(self.x_train, self.y_train,\n",
    "                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],\n",
    "                **para['fit_params'])\n",
    "        pred = reg.predict(self.x_test)\n",
    "        loss = para['loss_func'](self.y_test, pred)\n",
    "        return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "f3pCUcQrafVk",
    "outputId": "2ffe71ba-1a81-4361-d069-c75a2d9c434e"
   },
   "outputs": [],
   "source": [
    "# Data loading \n",
    "\n",
    "base_dir = '../data/.raw/GFS_data'\n",
    "power_csv = '../data/processed/power_data.csv'\n",
    "\n",
    "gfs_data_dict = {\n",
    " 'CAPEsfc': [23283, 23400], 'CLWMRprs': [3042, 6084], 'GUSTsfc': [22815, 22932],\n",
    " 'HGTprs': [0, 3042], 'HGTsfc': [21294, 21411], 'MSLETmsl': [21411, 21528], 'PWATclm': [21528, 21645],\n",
    " 'RH2m': [21645, 21762], 'RHprs': [6084, 9126], 'SPFH2m': [23400, 23517], 'SPFH80m': [23517, 23634],\n",
    " 'TMP2m': [23049, 23166], 'TMPprs': [18252, 21294], 'TMPsfc': [22932, 23049], 'UGRD100m': [21879, 21996],\n",
    " 'UGRD10m': [22581, 22698], 'UGRD80m': [22230, 22347], 'UGRDprs': [12168, 15210], 'VGRD100m': [21996, 22113],\n",
    " 'VGRD10m': [22698, 22815], 'VGRD80m': [22347, 22464], 'VGRDprs': [15210, 18252], 'Vel100m': [21762, 21879],\n",
    " 'Vel10m': [22464, 22581], 'Vel80m': [22113, 22230], 'Velprs': [9126, 12168], 'no4LFTXsfc': [23166, 23283]\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "eDuWIg5HbNIi",
    "outputId": "a26f8bb8-453f-49cf-817c-200d74377858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.651475191116333\n",
      "Elapsed time: 2.965580701828003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selection of variables and pre-processing\n",
    "list_var = [\"RHprs\", \"Velprs\", \"TMPprs\", \"Vel100m\",\"Vel80m\", \"TMPsfc\", \"SPFH80m\"]\n",
    "\n",
    "lista_dates = get_date(base_dir)\n",
    "\n",
    "variables_ready = get_variables(base_dir, list_var, gfs_data_dict, nz=5)\n",
    "\n",
    "\n",
    "df_power = pd.read_csv(power_csv)\n",
    "#when windows\n",
    "#df_power = pd.read_csv(power_csv, encoding='utf8', sep='\\t')\n",
    "\n",
    "\n",
    "df_gfs = pd.DataFrame(data=variables_ready, index=lista_dates, columns=list_var)\n",
    "df_power['date'] =  pd.to_datetime(df_power['date'], format='%d/%m/%Y %H:%M')\n",
    "df_power = df_power.set_index(\"date\")\n",
    "\n",
    "df_gfs.sort_index(axis=0, level=None, ascending=True, inplace=True)\n",
    "df_gfs = df_gfs.loc[:'31/12/2016 00:00']\n",
    "df_power.sort_index(axis=0, level=None, ascending=True, inplace=True)\n",
    "df_power = df_power.loc[:'31/12/2016 00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Rai_n-J8bkqB",
    "outputId": "aa54198f-2bf7-49ad-c673-270fa28492da"
   },
   "outputs": [],
   "source": [
    "# df intersection based on dates\n",
    "trained = df_power.merge(df_gfs, left_index=True, right_index=True) \n",
    "\n",
    "# Data preparation for model train\n",
    "trained = df_power.merge(df_gfs, left_index=True, right_index=True) # df intersection based on dates\n",
    "\n",
    "df_X = trained[[x for x in trained.columns if x != 'Production']]\n",
    "\n",
    "X = setup_x(df_X)\n",
    "y = pd.DataFrame(trained[\"Production\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "M6jwPJTebtVa",
    "outputId": "c44c9950-8968-418d-c0d7-b8defd53af69"
   },
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "#LGBMRegressor\n",
    "gbm0 = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    num_leaves=60,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    n_jobs = 12\n",
    "    )\n",
    "\n",
    "print(\"Fitting LGBMRegressor model...\")\n",
    "gbm_fit = gbm0.fit(X_train, y_train, eval_metric='rmse')\n",
    "print(\"Finished fitting LGBMRegressor model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "Jsf3ROz4b8ks",
    "outputId": "6a2ca58e-cc3c-46a6-bc4b-058ab767abed"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predict_lightGBM = gbm0.predict(X_test)\n",
    "\n",
    "y_trained = np.array(trained['Production'])\n",
    "y_pred = predict_lightGBM\n",
    "y_truth = np.array(y_test)\n",
    "\n",
    "print('explained_variance_score', explained_variance_score(y_truth, y_pred))\n",
    "print('max_error', max_error(y_truth, y_pred))\n",
    "print('mean_absolute_error', mean_absolute_error(y_truth, y_pred))\n",
    "print('mean_squared_error', mean_squared_error(y_truth, y_pred))\n",
    "print('mean_squared_log_error', mean_squared_log_error(y_truth**2, y_pred**2))\n",
    "print('median_absolute_error', median_absolute_error(y_truth, y_pred))\n",
    "print('r2_score', r2_score(y_truth, y_pred))\n",
    "print('rmse', rmse(y_truth, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "gKcCuBwycuO4",
    "outputId": "e3541962-cacd-4cc6-a3ec-9557cca61c63"
   },
   "outputs": [],
   "source": [
    "optimization_dict = {'max_depth': [2,4,6],\n",
    "                     'n_estimators': [50,100,200]}\n",
    "\n",
    "model_gbm = GridSearchCV(gbm0, optimization_dict, \n",
    "                     scoring='neg_mean_absolute_error', verbose=1)\n",
    "\n",
    "model_gbm.fit(X_train, y_train)\n",
    "print(\"GBM best score\", model_gbm.best_score_)\n",
    "print(\"GBM best params\", model_gbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KeUMW0uGdOBI",
    "outputId": "5da5391c-d62d-41c3-a51d-3cc6ede89484"
   },
   "outputs": [],
   "source": [
    "num_train, num_feature = X_train.shape\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 40,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# generate feature names\n",
    "feature_name = ['feature_' + str(col) for col in range(num_feature)]\n",
    "\n",
    "print('Starting training...')\n",
    "# feature_name and categorical_feature\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_train,  # eval training data\n",
    "                feature_name=feature_name,\n",
    "                categorical_feature=[21])\n",
    "\n",
    "print('Finished first 10 rounds...')\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Dumping model to JSON...')\n",
    "# dump model to JSON (and save to file)\n",
    "model_json = gbm.dump_model()\n",
    "\n",
    "with open('model.json', 'w+') as f:\n",
    "    json.dump(model_json, f, indent=4)\n",
    "\n",
    "print('Loading model to predict...')\n",
    "# load model to predict\n",
    "bst = lgb.Booster(model_file='model.txt')\n",
    "# can only predict with the best iteration (or the saving iteration)\n",
    "y_pred = bst.predict(X_test)\n",
    "# eval with loaded model \n",
    "print(\"The rmse of loaded model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CHUO81Ead0n7",
    "outputId": "607c7adb-531b-4b2f-df74-c3f2638ac1b5"
   },
   "outputs": [],
   "source": [
    "# dump model with pickle\n",
    "with open('model.pkl', 'wb') as fout:\n",
    "    pickle.dump(gbm, fout)\n",
    "# load model with pickle to predict\n",
    "with open('model.pkl', 'rb') as fin:\n",
    "    pkl_bst = pickle.load(fin)\n",
    "# can predict with any iteration when loaded in pickle way\n",
    "y_pred = pkl_bst.predict(X_test, num_iteration=7)\n",
    "# eval with loaded model\n",
    "print(\"The rmse of pickled model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4N85QHTieki4"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#################################################################\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "Sc_Ytyr5RiqC",
    "outputId": "26f3000e-634f-4336-f46a-a2b4451ca183"
   },
   "outputs": [],
   "source": [
    "# SECOND ROUND\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 10\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 10\n",
    "\n",
    "clf = lgb.train(params, lgb_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ii9xNkj1R5e4",
    "outputId": "19b7824a-ba8b-44d5-b6c9-905c7711f788"
   },
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred=clf.predict(X_test)\n",
    "#convert into binary values\n",
    "for i in range(0,99):\n",
    "    if y_pred[i]>=.5:       # setting threshold to .5\n",
    "        y_pred[i]=1\n",
    "    else:  \n",
    "        y_pred[i]=0\n",
    "print(\"The rmse of pickled model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "HhAVTyWISDQk",
    "outputId": "19496d16-ba00-48fc-83d9-e264c1c3bd7d"
   },
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "def f(x):\n",
    "    return {'loss': x ** 2 - x, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperopt optimization\n",
    "trials = Trials()\n",
    "result = fmin(\n",
    "    fn=f,                           # objective function\n",
    "    space=hp.uniform('x', -1, 1),   # parameter space\n",
    "    algo=tpe.suggest,               # surrogate algorithm\n",
    "    max_evals=500,                  # no. of evaluations\n",
    "    trials=trials                   # trials object that keeps track of the sample results (optional)\n",
    ")\n",
    "\n",
    "# Print the optimized parameters\n",
    "print(result)   # {'x': 0.5000833960783931}\n",
    "\n",
    "# Extract and plot the trials \n",
    "x = trials.vals['x']\n",
    "y = [x['loss'] for x in trials.results]\n",
    "plt.pyplot.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oM9hrUFSb9C"
   },
   "outputs": [],
   "source": [
    "\n",
    "# XGB parameters\n",
    "xgb_reg_params = {\n",
    "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(5, 16, 1, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
    "    'n_estimators':     100,\n",
    "}\n",
    "xgb_fit_params = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "xgb_para = dict()\n",
    "xgb_para['reg_params'] = xgb_reg_params\n",
    "xgb_para['fit_params'] = xgb_fit_params\n",
    "xgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n",
    "\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_reg_params = {\n",
    "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(5, 16, 1, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
    "    'n_estimators':     100,\n",
    "}\n",
    "lgb_fit_params = {\n",
    "    'eval_metric': 'l2',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "lgb_para = dict()\n",
    "lgb_para['reg_params'] = lgb_reg_params\n",
    "lgb_para['fit_params'] = lgb_fit_params\n",
    "lgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bB3gx3cITf8x",
    "outputId": "7ecadfd6-cfdf-487f-a554-9753696a8f63"
   },
   "outputs": [],
   "source": [
    "# We now apply the hiperoptimization class for both XGB and LightGBM models\n",
    "\n",
    "obj = HPOpt(X_train, X_test, y_train, y_test)\n",
    "xgb_opt = obj.process(fn_name='xgb_reg', space=xgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JRNDqQCyhi11",
    "outputId": "b7f72745-a40b-4970-9c42-d805872991ef"
   },
   "outputs": [],
   "source": [
    "lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iz-yiDbOkFmK"
   },
   "outputs": [],
   "source": [
    "# XGB RMSE -> 51914.20\n",
    "# LightGBM -> 51753.51\n",
    "\n",
    "# With a very small difference, we select our LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "eolo_lightgbm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
