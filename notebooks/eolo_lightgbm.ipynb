{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eolo_lightgbm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4v5RAJzaNOV",
        "colab_type": "code",
        "outputId": "192b92d0-e3f4-48d6-a26d-3c44b25c7e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbuvQeHIaecX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from functools import wraps\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import matplotlib as plt\n",
        "import folium\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except BaseException:\n",
        "    import pickle\n",
        "\n",
        "# Modeling\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Evaluation of the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error, median_absolute_error\n",
        "from sklearn.metrics import r2_score#, mean_gamma_deviance# mean_poisson_deviance, mean_gamma_deviance\n",
        "\n",
        "def timer(f):\n",
        "    @wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time()\n",
        "        result = f(*args, **kwargs)\n",
        "        end = time()\n",
        "        print('Elapsed time: {}'.format(end-start))\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# rmse\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_date(base_dir):\n",
        "    new_time = []\n",
        "    for file in listdir(base_dir):\n",
        "        file_path = f'{base_dir}/{file}'\n",
        "        match=file.split(\"_\")[1]\n",
        "        date = pd.to_datetime(match, format = \"%Y%m%d%H\").strftime('%d/%m/%Y')\n",
        "        time = (datetime.strptime(match, \"%Y%m%d%H\") + timedelta(hours=6)).strftime('%H:%M')\n",
        "        new_time.append(date + \" \" + time)\n",
        "    return new_time\n",
        "\n",
        "\n",
        "@timer\n",
        "def get_variables(base_dir, var_list, diccionario, nz=26):\n",
        "  d3_var = [\"HGTprs\", \"CLWMRprs\", \"RHprs\",\"Velprs\",\"UGRDprs\",\"VGRDprs\",\"TMPprs\"]\n",
        "  d2_var = [\"HGTsfc\", \"MSLETmsl\", \"PWATclm\", \"RH2m\", \"Vel100m\", \"UGRD100m\", \"VGRD100m\",\n",
        "            \"Vel80m\", \"UGRD80m\", \"VGRD80m\", \"Vel10m\", \"UGRD10m\", \"VGRD10m\", \"GUSTsfc\",\n",
        "            \"TMPsfc\", \"TMP2m\", \"no4LFTXsfc\", \"CAPEsfc\", \"SPFH2m\", \"SPFH80m\"]\n",
        "\n",
        "  lst = []\n",
        "  \n",
        "  for file in listdir(base_dir):\n",
        "    file_path = f'{base_dir}/{file}'\n",
        "    e_file = []\n",
        "    for key, value in diccionario.items():\n",
        "    \n",
        "      if key in set(var_list).intersection(d3_var): #d3_var:\n",
        "        corte = value[0] + int(((value[1])/26)*nz)\n",
        "        e_file.append(np.fromfile(file_path, dtype=np.float32)[value[0]:corte])\n",
        "\n",
        "      elif key in set(var_list).intersection(d2_var):#d2_var:\n",
        "        e_file.append(np.fromfile(file_path, dtype=np.float32)[value[0]:value[1]])\n",
        "    lst.append(e_file)\n",
        "  \n",
        "  return lst\n",
        "\n",
        "@timer\n",
        "def setup_x(dataframe):\n",
        "  \"\"\"Flat variables values for model training\"\"\"\n",
        "  dataframe.reset_index(level=0, inplace=True)\n",
        "  row_list =[] \n",
        "  for index, rows in dataframe.iterrows(): \n",
        "      my_list = [rows.RHprs, rows.Velprs, rows.TMPprs, rows.Vel100m, rows.Vel80m,rows.TMPsfc, rows.SPFH80m]\n",
        "      row_list.append(my_list) \n",
        "\n",
        "  a = [np.concatenate(row_list[i]) for i in range(len(row_list))]\n",
        "  train_ = pd.DataFrame(a, index=dataframe[\"index\"])\n",
        "  return train_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3pCUcQrafVk",
        "colab_type": "code",
        "outputId": "2ffe71ba-1a81-4361-d069-c75a2d9c434e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Data loading \n",
        "print(\"Reading the data...\")\n",
        "base_dir = '/content/drive/My Drive/datos/GFS_data'\n",
        "power_csv = '/content/pow_data.csv'#\n",
        "#power_csv=\"/content/power_data.csv\"\n",
        "gfs_dict_path = '/content/power_data.csv'\n",
        "\n",
        "gfs_data_dict = {\n",
        " 'CAPEsfc': [23283, 23400], 'CLWMRprs': [3042, 6084], 'GUSTsfc': [22815, 22932],\n",
        " 'HGTprs': [0, 3042], 'HGTsfc': [21294, 21411], 'MSLETmsl': [21411, 21528], 'PWATclm': [21528, 21645],\n",
        " 'RH2m': [21645, 21762], 'RHprs': [6084, 9126], 'SPFH2m': [23400, 23517], 'SPFH80m': [23517, 23634],\n",
        " 'TMP2m': [23049, 23166], 'TMPprs': [18252, 21294], 'TMPsfc': [22932, 23049], 'UGRD100m': [21879, 21996],\n",
        " 'UGRD10m': [22581, 22698], 'UGRD80m': [22230, 22347], 'UGRDprs': [12168, 15210], 'VGRD100m': [21996, 22113],\n",
        " 'VGRD10m': [22698, 22815], 'VGRD80m': [22347, 22464], 'VGRDprs': [15210, 18252], 'Vel100m': [21762, 21879],\n",
        " 'Vel10m': [22464, 22581], 'Vel80m': [22113, 22230], 'Velprs': [9126, 12168], 'no4LFTXsfc': [23166, 23283]\n",
        " }\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDuWIg5HbNIi",
        "colab_type": "code",
        "outputId": "a26f8bb8-453f-49cf-817c-200d74377858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "# Selection of variables and pre-processing\n",
        "list_var = [\"RHprs\", \"Velprs\", \"TMPprs\", \"Vel100m\",\"Vel80m\", \"TMPsfc\", \"SPFH80m\"]\n",
        "\n",
        "lista_dates = get_date(base_dir)\n",
        "\n",
        "variables_ready = get_variables(base_dir, list_var, gfs_data_dict, nz=5)\n",
        "\n",
        "## Set as DF\n",
        "#when slimbook\n",
        "df_power = pd.read_csv(power_csv)\n",
        "#df_power = pd.read_csv('/content/power_data.csv')#, encoding='utf8', sep='\\t')\n",
        "#when windows\n",
        "df_power = pd.read_csv(power_csv, encoding='utf8', sep='\\t')\n",
        "\n",
        "\n",
        "df_gfs = pd.DataFrame(data=variables_ready, index=lista_dates, columns=list_var)\n",
        "#df_power = pd.read_csv(power_csv, encoding='utf8')\n",
        "df_power['date'] =  pd.to_datetime(df_power['date'], format='%d/%m/%Y %H:%M')\n",
        "df_power = df_power.set_index(\"date\")\n",
        "\n",
        "df_gfs.sort_index(axis=0, level=None, ascending=True, inplace=True)\n",
        "df_gfs = df_gfs.loc[:'31/12/2016 00:00']\n",
        "df_power.sort_index(axis=0, level=None, ascending=True, inplace=True)\n",
        "df_power = df_power.loc[:'31/12/2016 00:00']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time: 20.98387360572815\n",
            "Elapsed time: 2606.9707458019257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rai_n-J8bkqB",
        "colab_type": "code",
        "outputId": "aa54198f-2bf7-49ad-c673-270fa28492da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# df intersection based on dates\n",
        "trained = df_power.merge(df_gfs, left_index=True, right_index=True) \n",
        "\n",
        "# Data preparation for model train\n",
        "trained = df_power.merge(df_gfs, left_index=True, right_index=True) # df intersection based on dates\n",
        "\n",
        "df_X = trained[[x for x in trained.columns if x != 'Production']]\n",
        "\n",
        "X = setup_x(df_X)\n",
        "y = pd.DataFrame(trained[\"Production\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time: 9.598525285720825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6jwPJTebtVa",
        "colab_type": "code",
        "outputId": "c44c9950-8968-418d-c0d7-b8defd53af69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "#LGBMRegressor\n",
        "gbm0 = lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    num_leaves=60,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=1000)\n",
        "\n",
        "print(\"Fitting LGBMRegressor model...\")\n",
        "gbm_fit = gbm0.fit(X_train, y_train, eval_metric='rmse')\n",
        "print(\"Finished fitting LGBMRegressor model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting LGBMRegressor model...\n",
            "Finished fitting LGBMRegressor model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsf3ROz4b8ks",
        "colab_type": "code",
        "outputId": "6a2ca58e-cc3c-46a6-bc4b-058ab767abed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# Prediction\n",
        "predict_lightGBM = gbm0.predict(X_test)\n",
        "\n",
        "y_trained = np.array(trained['Production'])\n",
        "y_pred = predict_lightGBM\n",
        "y_truth = np.array(y_test)\n",
        "print('explained_variance_score', explained_variance_score(y_truth, y_pred))\n",
        "print('max_error', max_error(y_truth, y_pred))\n",
        "print('mean_absolute_error', mean_absolute_error(y_truth, y_pred))\n",
        "print('mean_squared_error', mean_squared_error(y_truth, y_pred))\n",
        "print('mean_squared_log_error', mean_squared_log_error(y_truth**2, y_pred**2))\n",
        "print('median_absolute_error', median_absolute_error(y_truth, y_pred))\n",
        "print('r2_score', r2_score(y_truth, y_pred))\n",
        "print('rmse', rmse(y_truth, y_pred))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "explained_variance_score 0.29043962199610385\n",
            "max_error 169229.4844695173\n",
            "mean_absolute_error 42396.972129664144\n",
            "mean_squared_error 2888827639.8226266\n",
            "mean_squared_log_error 70.51175845464078\n",
            "median_absolute_error 34414.01404656979\n",
            "r2_score 0.2896459877538511\n",
            "rmse 53747.8152097611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKcCuBwycuO4",
        "colab_type": "code",
        "outputId": "e3541962-cacd-4cc6-a3ec-9557cca61c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "optimization_dict = {'max_depth': [2,4,6],\n",
        "                     'n_estimators': [50,100,200]}\n",
        "\n",
        "model_gbm = GridSearchCV(gbm0, optimization_dict, \n",
        "                     scoring='neg_mean_absolute_error', verbose=1)\n",
        "\n",
        "model_gbm.fit(X_train, y_train)\n",
        "print(\"GBM best score\", model_gbm.best_score_)\n",
        "print(\"GBM best params\", model_gbm.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed: 22.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GBM best score -42349.12652737538\n",
            "GBM best params {'max_depth': 6, 'n_estimators': 200}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeUMW0uGdOBI",
        "colab_type": "code",
        "outputId": "5da5391c-d62d-41c3-a51d-3cc6ede89484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_train, num_feature = X_train.shape\n",
        "\n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, free_raw_data=False)\n",
        "\n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'mse',\n",
        "    'num_leaves': 40,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# generate feature names\n",
        "feature_name = ['feature_' + str(col) for col in range(num_feature)]\n",
        "\n",
        "print('Starting training...')\n",
        "# feature_name and categorical_feature\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=100,\n",
        "                valid_sets=lgb_train,  # eval training data\n",
        "                feature_name=feature_name,\n",
        "                categorical_feature=[21])\n",
        "\n",
        "print('Finished first 10 rounds...')\n",
        "print('Saving model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')\n",
        "\n",
        "print('Dumping model to JSON...')\n",
        "# dump model to JSON (and save to file)\n",
        "model_json = gbm.dump_model()\n",
        "\n",
        "with open('model.json', 'w+') as f:\n",
        "    json.dump(model_json, f, indent=4)\n",
        "\n",
        "# feature importances\n",
        "print('Feature importances:', list(gbm.feature_importance()))\n",
        "\n",
        "print('Loading model to predict...')\n",
        "# load model to predict\n",
        "bst = lgb.Booster(model_file='model.txt')\n",
        "# can only predict with the best iteration (or the saving iteration)\n",
        "y_pred = bst.predict(X_test)\n",
        "# eval with loaded model \n",
        "print(\"The rmse of loaded model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
            "New categorical_feature is [21]\n",
            "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1]\ttraining's l2: 6.59782e+09\n",
            "[2]\ttraining's l2: 6.59782e+09\n",
            "[3]\ttraining's l2: 6.59782e+09\n",
            "[4]\ttraining's l2: 6.59782e+09\n",
            "[5]\ttraining's l2: 6.59782e+09\n",
            "[6]\ttraining's l2: 6.59782e+09\n",
            "[7]\ttraining's l2: 6.59782e+09\n",
            "[8]\ttraining's l2: 6.59782e+09\n",
            "[9]\ttraining's l2: 6.59782e+09\n",
            "[10]\ttraining's l2: 6.59782e+09\n",
            "[11]\ttraining's l2: 6.59782e+09\n",
            "[12]\ttraining's l2: 6.59782e+09\n",
            "[13]\ttraining's l2: 6.59782e+09\n",
            "[14]\ttraining's l2: 6.59782e+09\n",
            "[15]\ttraining's l2: 6.59782e+09\n",
            "[16]\ttraining's l2: 6.59782e+09\n",
            "[17]\ttraining's l2: 6.59782e+09\n",
            "[18]\ttraining's l2: 6.59782e+09\n",
            "[19]\ttraining's l2: 6.59782e+09\n",
            "[20]\ttraining's l2: 6.59782e+09\n",
            "[21]\ttraining's l2: 6.59782e+09\n",
            "[22]\ttraining's l2: 6.59782e+09\n",
            "[23]\ttraining's l2: 6.59782e+09\n",
            "[24]\ttraining's l2: 6.59782e+09\n",
            "[25]\ttraining's l2: 6.59782e+09\n",
            "[26]\ttraining's l2: 6.59782e+09\n",
            "[27]\ttraining's l2: 6.59782e+09\n",
            "[28]\ttraining's l2: 6.59782e+09\n",
            "[29]\ttraining's l2: 6.59782e+09\n",
            "[30]\ttraining's l2: 6.59782e+09\n",
            "[31]\ttraining's l2: 6.59782e+09\n",
            "[32]\ttraining's l2: 6.59782e+09\n",
            "[33]\ttraining's l2: 6.59782e+09\n",
            "[34]\ttraining's l2: 6.59782e+09\n",
            "[35]\ttraining's l2: 6.59781e+09\n",
            "[36]\ttraining's l2: 6.59781e+09\n",
            "[37]\ttraining's l2: 6.59781e+09\n",
            "[38]\ttraining's l2: 6.59781e+09\n",
            "[39]\ttraining's l2: 6.59781e+09\n",
            "[40]\ttraining's l2: 6.59781e+09\n",
            "[41]\ttraining's l2: 6.59781e+09\n",
            "[42]\ttraining's l2: 6.59781e+09\n",
            "[43]\ttraining's l2: 6.59781e+09\n",
            "[44]\ttraining's l2: 6.59781e+09\n",
            "[45]\ttraining's l2: 6.59781e+09\n",
            "[46]\ttraining's l2: 6.59781e+09\n",
            "[47]\ttraining's l2: 6.59781e+09\n",
            "[48]\ttraining's l2: 6.59781e+09\n",
            "[49]\ttraining's l2: 6.59781e+09\n",
            "[50]\ttraining's l2: 6.59781e+09\n",
            "[51]\ttraining's l2: 6.59781e+09\n",
            "[52]\ttraining's l2: 6.59781e+09\n",
            "[53]\ttraining's l2: 6.59781e+09\n",
            "[54]\ttraining's l2: 6.59781e+09\n",
            "[55]\ttraining's l2: 6.59781e+09\n",
            "[56]\ttraining's l2: 6.59781e+09\n",
            "[57]\ttraining's l2: 6.59781e+09\n",
            "[58]\ttraining's l2: 6.59781e+09\n",
            "[59]\ttraining's l2: 6.59781e+09\n",
            "[60]\ttraining's l2: 6.59781e+09\n",
            "[61]\ttraining's l2: 6.59781e+09\n",
            "[62]\ttraining's l2: 6.59781e+09\n",
            "[63]\ttraining's l2: 6.59781e+09\n",
            "[64]\ttraining's l2: 6.59781e+09\n",
            "[65]\ttraining's l2: 6.59781e+09\n",
            "[66]\ttraining's l2: 6.59781e+09\n",
            "[67]\ttraining's l2: 6.59781e+09\n",
            "[68]\ttraining's l2: 6.59781e+09\n",
            "[69]\ttraining's l2: 6.59781e+09\n",
            "[70]\ttraining's l2: 6.59781e+09\n",
            "[71]\ttraining's l2: 6.59781e+09\n",
            "[72]\ttraining's l2: 6.59781e+09\n",
            "[73]\ttraining's l2: 6.59781e+09\n",
            "[74]\ttraining's l2: 6.59781e+09\n",
            "[75]\ttraining's l2: 6.59781e+09\n",
            "[76]\ttraining's l2: 6.59781e+09\n",
            "[77]\ttraining's l2: 6.59781e+09\n",
            "[78]\ttraining's l2: 6.59781e+09\n",
            "[79]\ttraining's l2: 6.59781e+09\n",
            "[80]\ttraining's l2: 6.59781e+09\n",
            "[81]\ttraining's l2: 6.59781e+09\n",
            "[82]\ttraining's l2: 6.59781e+09\n",
            "[83]\ttraining's l2: 6.59781e+09\n",
            "[84]\ttraining's l2: 6.59781e+09\n",
            "[85]\ttraining's l2: 6.59781e+09\n",
            "[86]\ttraining's l2: 6.59781e+09\n",
            "[87]\ttraining's l2: 6.59781e+09\n",
            "[88]\ttraining's l2: 6.59781e+09\n",
            "[89]\ttraining's l2: 6.59781e+09\n",
            "[90]\ttraining's l2: 6.59781e+09\n",
            "[91]\ttraining's l2: 6.59781e+09\n",
            "[92]\ttraining's l2: 6.59781e+09\n",
            "[93]\ttraining's l2: 6.59781e+09\n",
            "[94]\ttraining's l2: 6.59781e+09\n",
            "[95]\ttraining's l2: 6.59781e+09\n",
            "[96]\ttraining's l2: 6.59781e+09\n",
            "[97]\ttraining's l2: 6.59781e+09\n",
            "[98]\ttraining's l2: 6.59781e+09\n",
            "[99]\ttraining's l2: 6.59781e+09\n",
            "[100]\ttraining's l2: 6.59781e+09\n",
            "Finished first 10 rounds...\n",
            "Saving model...\n",
            "Dumping model to JSON...\n",
            "Feature importances: [1, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 87, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 4, 1, 0, 0, 0, 0, 2, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 2, 0, 1, 2, 3, 1, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 0, 0, 19, 1, 0, 0, 0, 0, 1, 2, 1, 1, 8, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 3, 0, 5, 4, 0, 0, 3, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 1, 1, 2, 0, 3, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 6, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 1, 2, 5, 3, 0, 1, 1, 0, 3, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 4, 1, 3, 1, 3, 0, 0, 0, 0, 2, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 2, 0, 4, 0, 2, 1, 3, 1, 2, 0, 0, 0, 0, 5, 0, 3, 1, 1, 0, 0, 0, 4, 0, 1, 0, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 2, 1, 3, 2, 1, 1, 0, 1, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 2, 0, 3, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 3, 0, 4, 0, 0, 4, 3, 1, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 1, 3, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 3, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 0, 2, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 1, 2, 3, 0, 3, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 6, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 3, 2, 0, 1, 0, 3, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 3, 1, 0, 0, 0, 1, 4, 0, 2, 2, 0, 1, 1, 0, 1, 1, 1, 3, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 8, 3, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 3, 3, 1, 0, 0, 0, 1, 1, 0, 3, 0, 0, 3, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 1, 3, 2, 4, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 1, 0, 0, 1, 0, 0, 3, 0, 1, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 5, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 2, 1, 1, 3, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 4, 0, 3, 0, 0, 0, 0, 2, 1, 0, 3, 2, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 0, 1, 3, 2, 0, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 3, 0, 3, 0, 0, 3, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 2, 2, 1, 1, 3, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 3, 0, 1, 1, 0, 0, 1, 1, 4, 1, 0, 0, 1, 1, 0, 4, 1, 0, 0, 0, 2, 6, 0, 0, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 4, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 0, 0, 3, 1, 1, 4, 1, 1, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 4, 1, 0, 0, 2, 2, 1, 1, 2, 1, 0, 2, 0, 0, 1, 2, 1, 1, 0, 2, 1, 0, 1, 0, 2, 0, 0, 3, 2, 0, 0, 3, 1, 1, 0, 0, 1, 2, 1, 2, 2, 0, 3, 1, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 4, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 4, 0, 0, 2, 2, 0, 5, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 4, 0, 1, 1, 1, 0, 0, 0, 3, 0, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 3, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 1, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 0, 1, 0, 4, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 3, 1, 0, 3, 0, 3, 1, 0, 2, 0, 2, 1, 2, 2, 5, 0, 0, 0, 1, 0, 1, 3, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 1, 0, 1, 1, 1, 2, 2, 3, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 6, 1, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 4, 1, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 3, 0, 0, 0, 0, 1, 1, 1, 0, 1, 2, 3, 2, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 1, 3, 3, 1, 1, 1, 4, 0, 0, 1, 1, 0, 0, 0, 3, 2, 0, 1, 0, 0, 0, 2, 0, 0, 1, 3, 0, 2, 3, 1, 0, 0, 0, 0, 1, 2, 2, 3, 2, 2, 3, 1, 1, 2, 1, 0, 0, 0, 1, 0, 1, 2, 0, 2, 0, 5, 0, 0, 1, 0, 2, 1, 3, 0, 1, 1, 0, 0, 6, 3, 4, 6, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 1, 2, 0, 1, 2, 2, 3, 3, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 3, 0, 2, 2, 2, 1, 1, 3, 1, 1, 2, 1, 1, 1, 0, 0, 0, 3, 2, 2, 1, 1, 1, 0, 2, 5, 2, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 3, 3, 0, 4, 0, 1, 0, 1, 0, 1, 1, 0, 1, 4, 1, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 0, 0, 0, 2, 2, 1, 6, 0, 1, 0, 2, 1, 1, 2, 1, 3, 3, 0, 0, 2, 0, 2, 0, 0, 3, 3, 0, 2, 0, 1, 0, 3, 1, 1, 1, 0, 1, 2, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 3, 1, 3, 1, 0, 1, 2, 1, 2, 2, 0, 4, 1, 2, 2, 2, 3, 0, 0, 0, 0, 0, 4, 2, 4, 3, 0, 0, 2, 6, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 5, 1, 8, 2, 0, 0, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 8, 2, 0, 1, 3, 1, 2, 0, 2, 2, 5, 1, 1, 4, 5, 2, 2, 3, 1, 2, 0, 1, 0, 4, 2, 2, 4, 1, 3, 3, 6, 1, 3, 0, 1, 1, 0, 2, 3, 5, 3, 2, 3, 2, 2, 1, 1, 0, 1, 2, 4, 1, 1, 1, 3, 2, 1, 0, 1, 0, 1, 2, 4, 1, 0, 3, 4, 3, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 4, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 4, 0, 1, 4, 0, 1, 1, 2, 0, 2, 2, 0, 3, 2, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 2, 2, 3, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 1, 0, 4, 1, 3, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 0, 1, 1, 2, 0, 2, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 3, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 3, 0, 0, 1, 0, 0, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 2, 0, 3, 0, 0, 0, 6, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 3, 3, 5, 4, 0, 3, 0, 0, 3, 0, 2, 0, 4, 1, 1, 2, 0, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 2, 2, 1, 2, 0, 0, 1, 2, 3, 1, 4, 2, 0, 2, 1, 0, 2, 0, 3, 3, 4, 2, 1, 0, 3, 0, 4, 4, 1, 0, 0, 2, 2, 1, 1, 5, 0, 1, 0, 1, 1, 0, 5, 1, 0, 2, 0, 5, 8, 3, 2, 1, 0, 4, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 5, 3, 1, 0, 2, 1, 1, 2, 3, 3, 2, 1, 2, 0, 1, 1, 3, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 2, 4, 5, 2, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 5, 4, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 18, 12, 4, 3, 0, 0, 0, 0, 1, 0, 0, 1, 5, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 2, 1, 7, 3, 3, 0, 1, 1, 2, 2, 3, 3, 2, 0, 0, 0, 5, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 3, 1, 3, 0, 0, 3, 0, 0, 1, 0, 0, 1, 3, 2, 1, 1, 2, 0, 1, 2, 1, 2, 1, 0, 0, 0, 3, 0, 1, 1, 1, 1, 2, 3, 0, 1, 0, 0, 1, 3, 1, 1, 1, 3, 0, 3, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 5, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 2, 2, 3, 11, 3, 3, 2, 2, 0, 0, 0, 1, 3, 9, 5, 10, 3, 2, 1, 4, 0, 0, 0, 0, 2, 0, 0, 0, 1, 4, 4, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 4, 2, 2, 2, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 4, 1, 1, 0, 0, 5, 1, 1, 3, 1, 1, 0, 2, 1, 2, 0, 2, 2, 1, 0, 1, 1, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 2, 0, 3, 1, 2, 0, 0, 0, 1, 3, 2, 0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1, 0, 2, 1, 3, 2, 7, 0, 0, 0, 0, 1, 1, 1, 2, 0, 2, 1, 6, 0, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 4, 4, 3, 3, 4, 0, 0, 0, 3, 0, 0, 2, 2, 0, 1, 0, 4, 1, 1, 2, 0, 1, 2, 0, 0, 2, 1, 2, 2, 2, 3, 0, 1, 2, 1, 3, 1, 0, 1, 2, 1, 1, 0, 1, 3, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 1, 1, 4, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 3, 0, 0, 2, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 2, 4, 0, 1, 2, 4, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 4, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 2, 1, 4, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2, 3, 0, 0, 1, 0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 1, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 2, 4, 1, 0, 0, 3, 0, 1, 2, 1, 1, 3, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 6, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 1]\n",
            "Loading model to predict...\n",
            "The rmse of loaded model's prediction is: 86209.39417363291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHUO81Ead0n7",
        "colab_type": "code",
        "outputId": "607c7adb-531b-4b2f-df74-c3f2638ac1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# dump model with pickle\n",
        "with open('model.pkl', 'wb') as fout:\n",
        "    pickle.dump(gbm, fout)\n",
        "# load model with pickle to predict\n",
        "with open('model.pkl', 'rb') as fin:\n",
        "    pkl_bst = pickle.load(fin)\n",
        "# can predict with any iteration when loaded in pickle way\n",
        "y_pred = pkl_bst.predict(X_test, num_iteration=7)\n",
        "# eval with loaded model\n",
        "print(\"The rmse of pickled model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rmse of pickled model's prediction is: 86209.45741192876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N85QHTieki4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################\n",
        "#################################################################\n",
        "#################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc_Ytyr5RiqC",
        "colab_type": "code",
        "outputId": "26f3000e-634f-4336-f46a-a2b4451ca183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# SECOND ROUND\n",
        "params = {}\n",
        "params['learning_rate'] = 0.003\n",
        "params['boosting_type'] = 'gbdt'\n",
        "params['objective'] = 'binary'\n",
        "params['metric'] = 'binary_logloss'\n",
        "params['sub_feature'] = 0.5\n",
        "params['num_leaves'] = 10\n",
        "params['min_data'] = 50\n",
        "params['max_depth'] = 10\n",
        "\n",
        "clf = lgb.train(params, lgb_train, 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii9xNkj1R5e4",
        "colab_type": "code",
        "outputId": "19b7824a-ba8b-44d5-b6c9-905c7711f788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Prediction\n",
        "y_pred=clf.predict(X_test)\n",
        "#convert into binary values\n",
        "for i in range(0,99):\n",
        "  if y_pred[i]>=.5:       # setting threshold to .5\n",
        "    y_pred[i]=1\n",
        "  else:  \n",
        "    y_pred[i]=0\n",
        "print(\"The rmse of pickled model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rmse of pickled model's prediction is: 86209.4440394231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAVTyWISDQk",
        "colab_type": "code",
        "outputId": "19496d16-ba00-48fc-83d9-e264c1c3bd7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define objective function\n",
        "def f(x):\n",
        "    return {'loss': x ** 2 - x, 'status': STATUS_OK}\n",
        "\n",
        "# Run hyperopt optimization\n",
        "trials = Trials()\n",
        "result = fmin(\n",
        "    fn=f,                           # objective function\n",
        "    space=hp.uniform('x', -1, 1),   # parameter space\n",
        "    algo=tpe.suggest,               # surrogate algorithm\n",
        "    max_evals=500,                  # no. of evaluations\n",
        "    trials=trials                   # trials object that keeps track of the sample results (optional)\n",
        ")\n",
        "\n",
        "# Print the optimized parameters\n",
        "print(result)   # {'x': 0.5000833960783931}\n",
        "\n",
        "# Extract and plot the trials \n",
        "x = trials.vals['x']\n",
        "y = [x['loss'] for x in trials.results]\n",
        "plt.scatter(x, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:02<00:00, 210.41it/s, best loss: -0.24999965642266697]\n",
            "{'x': 0.5005861547005761}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fbd470762e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oM9hrUFSb9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# XGB parameters\n",
        "xgb_reg_params = {\n",
        "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
        "    'max_depth':        hp.choice('max_depth',        np.arange(5, 16, 1, dtype=int)),\n",
        "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
        "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
        "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
        "    'n_estimators':     100,\n",
        "}\n",
        "xgb_fit_params = {\n",
        "    'eval_metric': 'rmse',\n",
        "    'early_stopping_rounds': 10,\n",
        "    'verbose': False\n",
        "}\n",
        "xgb_para = dict()\n",
        "xgb_para['reg_params'] = xgb_reg_params\n",
        "xgb_para['fit_params'] = xgb_fit_params\n",
        "xgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n",
        "\n",
        "\n",
        "# LightGBM parameters\n",
        "lgb_reg_params = {\n",
        "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
        "    'max_depth':        hp.choice('max_depth',        np.arange(5, 16, 1, dtype=int)),\n",
        "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
        "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
        "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
        "    'n_estimators':     100,\n",
        "}\n",
        "lgb_fit_params = {\n",
        "    'eval_metric': 'l2',\n",
        "    'early_stopping_rounds': 10,\n",
        "    'verbose': False\n",
        "}\n",
        "lgb_para = dict()\n",
        "lgb_para['reg_params'] = lgb_reg_params\n",
        "lgb_para['fit_params'] = lgb_fit_params\n",
        "lgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))\n",
        "\n",
        "\n",
        "# CatBoost parameters\n",
        "ctb_reg_params = {\n",
        "    'learning_rate':     hp.choice('learning_rate',     np.arange(0.05, 0.31, 0.05)),\n",
        "    'max_depth':         hp.choice('max_depth',         np.arange(5, 16, 1, dtype=int)),\n",
        "    'colsample_bylevel': hp.choice('colsample_bylevel', np.arange(0.3, 0.8, 0.1)),\n",
        "    'n_estimators':      100,\n",
        "    'eval_metric':       'RMSE',\n",
        "}\n",
        "ctb_fit_params = {\n",
        "    'early_stopping_rounds': 10,\n",
        "    'verbose': False\n",
        "}\n",
        "ctb_para = dict()\n",
        "ctb_para['reg_params'] = ctb_reg_params\n",
        "ctb_para['fit_params'] = ctb_fit_params\n",
        "ctb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGakaqyhTMiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "#import catboost as ctb\n",
        "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
        "\n",
        "#https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e\n",
        "\n",
        "@timer\n",
        "\n",
        "class HPOpt(object):\n",
        "\n",
        "    def __init__(self, x_train, x_test, y_train, y_test):\n",
        "        self.x_train = x_train\n",
        "        self.x_test  = x_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test  = y_test\n",
        "\n",
        "    def process(self, fn_name, space, trials, algo, max_evals):\n",
        "        fn = getattr(self, fn_name)\n",
        "        try:\n",
        "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
        "        except Exception as e:\n",
        "            return {'status': STATUS_FAIL,\n",
        "                    'exception': str(e)}\n",
        "        return result, trials\n",
        "\n",
        "    def xgb_reg(self, para):\n",
        "        reg = xgb.XGBRegressor(**para['reg_params'])\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def lgb_reg(self, para):\n",
        "        reg = lgb.LGBMRegressor(**para['reg_params'])\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def ctb_reg(self, para):\n",
        "        reg = ctb.CatBoostRegressor(**para['reg_params'])\n",
        "        return self.train_reg(reg, para)\n",
        "\n",
        "    def train_reg(self, reg, para):\n",
        "        reg.fit(self.x_train, self.y_train,\n",
        "                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],\n",
        "                **para['fit_params'])\n",
        "        pred = reg.predict(self.x_test)\n",
        "        loss = para['loss_func'](self.y_test, pred)\n",
        "        return {'loss': loss, 'status': STATUS_OK}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB3gx3cITf8x",
        "colab_type": "code",
        "outputId": "7ecadfd6-cfdf-487f-a554-9753696a8f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "obj = HPOpt(X_train, X_test, y_train, y_test)\n",
        "xgb_opt = obj.process(fn_name='xgb_reg', space=xgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time: 5.7220458984375e-06\n",
            "[11:17:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:18:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:19:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:19:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:20:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:21:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:22:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:22:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:23:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:26:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:28:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:30:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:31:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:35:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:38:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:42:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:44:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:46:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:46:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:47:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:48:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:52:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:55:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[11:58:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:01:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:04:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:07:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:10:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:14:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:16:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:18:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:19:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:20:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:22:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:23:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:25:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:26:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:27:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:30:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:32:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:33:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:33:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:37:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:42:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:43:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:44:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:45:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:45:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:46:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:47:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:48:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:50:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:51:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:52:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:53:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:56:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:57:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:58:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:02:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:07:25] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:08:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:09:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:11:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:12:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:13:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:16:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:17:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:18:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:19:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:20:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:22:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:24:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:24:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:30:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:31:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:35:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:36:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:39:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:42:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:46:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:48:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:51:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:55:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:58:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:00:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:01:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:01:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:03:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:06:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:07:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:10:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:10:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:14:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:15:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:16:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:18:03] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:22:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:24:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:25:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:28:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "100%|██████████| 100/100 [3:14:48<00:00, 162.29s/it, best loss: 51914.2042677069]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRNDqQCyhi11",
        "colab_type": "code",
        "outputId": "b7f72745-a40b-4970-9c42-d805872991ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lgb_opt = obj.process(fn_name='lgb_reg', space=lgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [1:52:11<00:00, 68.13s/it, best loss: 51753.51522429675]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvbDcr4-1Nc3",
        "colab_type": "code",
        "outputId": "442131af-f1de-4629-f119-71ddf840ff0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!pip install hpsklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hpsklearn\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/cb/61b99f73621e2692abd0e730f7888a9983d01f626868336fa1db1d57bc1e/hpsklearn-0.1.0.tar.gz\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (0.1.2)\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (1.17.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (0.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hpsklearn) (1.3.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (2.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (4.28.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hpsklearn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->hpsklearn) (0.14.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hpsklearn) (4.4.1)\n",
            "Building wheels for collected packages: hpsklearn\n",
            "  Building wheel for hpsklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hpsklearn: filename=hpsklearn-0.1.0-cp36-none-any.whl size=23913 sha256=463b143e8394f987ec4743eaf9da44e22f27308ea8a813cedfebcc61eea4c782\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/ee/c4/3c267cbf78f0905434ee36b915d97a20610ad3af7ff3c75852\n",
            "Successfully built hpsklearn\n",
            "Installing collected packages: nose, hpsklearn\n",
            "Successfully installed hpsklearn-0.1.0 nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XsmFTIvTs4l",
        "colab_type": "code",
        "outputId": "31189d2d-8275-4101-9434-04bb12a21251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "from hpsklearn import HyperoptEstimator, any_classifier\n",
        "#from sklearn.datasets import fetch_mldata\n",
        "from hyperopt import tpe\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Download the data and split into training and test sets\n",
        "#digits = fetch_mldata('MNIST original')\n",
        "\n",
        "#X = digits.data\n",
        "#y = digits.target\n",
        "\n",
        "#test_size = int( 0.2 * len( y ) )\n",
        "#np.random.seed(seed)\n",
        "#indices = np.random.permutation(len(X))\n",
        "#X_train = X[indices[:-test_size]]\n",
        "#y_train = y[indices[:-test_size]]\n",
        "#X_test  = X[indices[-test_size:]]\n",
        "#y_test  = y[indices[-test_size:]]\n",
        "\n",
        "estim = HyperoptEstimator(classifier=any_classifier('clf'),  \n",
        "                          algo=tpe.suggest, \n",
        "                          trial_timeout=300)\n",
        "\n",
        "estim.fit(X_train, y_train)\n",
        "\n",
        "print(estim.score( X_test, y_test ))\n",
        "# <<show score here>>\n",
        "print(estim.best_model())\n",
        "\n",
        "## https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARN: OMP_NUM_THREADS=None =>\n",
            "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n",
            "  0%|          | 0/1 [00:00<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2358672aa876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                           trial_timeout=300)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mestim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hpsklearn/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state, weights)\u001b[0m\n\u001b[1;32m    744\u001b[0m             increment = min(self.fit_increment,\n\u001b[1;32m    745\u001b[0m                             adjusted_max_evals - len(self.trials.trials))\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mfit_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hpsklearn/estimator.py\u001b[0m in \u001b[0;36mfit_iter\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state, weights, increment)\u001b[0m\n\u001b[1;32m    655\u001b[0m                               \u001b[0;31m#    so we notice them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                               \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                               \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# -- in case no success so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                              )\n\u001b[1;32m    659\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hpsklearn/estimator.py\u001b[0m in \u001b[0;36mfn_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mfn_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'return'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mfn_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;31m# -- remove potentially large objects from the rval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n            ...\\n            1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823],\\n           dtype='int64', length=1824)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz-yiDbOkFmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}