{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import xgboost as xgb\n",
    "\n",
    "def loading(file_path):\n",
    "    \"\"\"\n",
    "    Given GFS data in several .gra files, this function iterate over a given folder path importing and\n",
    "    organising its content in a dictionary format.\n",
    "    \"\"\"\n",
    "    total={}\n",
    "    file_data = []\n",
    "    new_time = []\n",
    "    print(\"Loading files and classifing\")\n",
    "    for root, subdirs, files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            array = np.fromfile(file_path +\"/\"+ file, dtype=np.float32)\n",
    "\n",
    "    for root, subdirs, files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            match=file.split(\"_\")[1]\n",
    "            date = pd.to_datetime(match, format = \"%Y%m%d%H\").strftime('%d/%m/%Y')\n",
    "            time = (datetime.strptime(match, \"%Y%m%d%H\") + timedelta(hours=6)).strftime('%H:%M')\n",
    "            new_time.append(date + \" \" + time)\n",
    "\n",
    "\n",
    "    for file in os.listdir(file_path):\n",
    "        start = 0\n",
    "        step_3d = 13*9*26\n",
    "        end = step_3d\n",
    "        features_3d = {\n",
    "                  \"HGTprs\": {'dimesiones': [13, 9, 26], 'data': None},\n",
    "                  \"CLWMRprs\": {'dimesiones': [13, 9, 26], 'data': None},\n",
    "                  \"RHprs\": {'dimesiones': [13, 9, 26], 'data': None},\n",
    "                  \"Velprs\": {'dimesiones': [13, 9, 26], 'data': None},\n",
    "                  \"UGRDprs\": {'dimesiones': [13, 9, 26], 'data': None},\n",
    "                  \"VGRDprs\": {'dimesiones': [13, 9, 26], 'data': None},\n",
    "                  \"TMPprs\": {'dimesiones': [13, 9, 26], 'data': None}\n",
    "               }\n",
    "\n",
    "        end = end - step_3d\n",
    "        step_2d = 13*9\n",
    "        end = end +step_2d\n",
    "        features_2d = {\n",
    "                   \"HGTsfc\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"MSLETmsl\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"PWATclm\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"RH2m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"Vel100m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"UGRD100m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"VGRD100m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"Vel80m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"UGRD80m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"VGRD80m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"Vel10m\":{'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"UGRD10m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"VGRD10m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"GUSTsfc\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"TMPsfc\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"TMP2m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"no4LFTXsfc\":{'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"CAPEsfc\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"SPFH2m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "                   \"SPFH80m\": {'dimesiones': [13, 9, 1], 'data': None},\n",
    "               }\n",
    "\n",
    "        size_3d = 13*9*26\n",
    "        array_3d = array[:size_3d*7]\n",
    "        for variable, length in zip(features_3d.keys(), range(len(features_3d))):\n",
    "            features_3d[variable][\"data\"] = array_3d[length*size_3d:(length +1)*size_3d]#.reshape((len(features_3d.keys()), corte))\n",
    "\n",
    "        size_2d = 13*9\n",
    "        array_2d = array[size_3d*7:]\n",
    "        for variable, length in zip(features_2d.keys(), range(len(features_2d))):\n",
    "            features_2d[variable][\"data\"] = array_2d[length*size_2d:(length +1)*size_2d]#.reshape((len(features_2d.keys()), corte))\n",
    "\n",
    "\n",
    "\n",
    "        file_data.append( {\n",
    "            \"file_name\": file,\n",
    "            #\"datetime\": new_time,\n",
    "            \"var_3d\": features_3d,\n",
    "            \"var_2d\":features_2d,\n",
    "        })\n",
    "\n",
    "    for i in range(len(new_time)):\n",
    "        total.update({new_time[i]:file_data[i]})\n",
    "\n",
    "    print(\"It's done!\")\n",
    "\n",
    "    return  total\n",
    "\n",
    "\n",
    "def get_var(main_dic, list_var, nz=26):\n",
    "    \"\"\"This function provides the selected variables in a nested dictionary with the given array\n",
    "    and level (consider that each level is around 50m heigth). Output is given as dictionary\n",
    "    :rtype: object\n",
    "    \"\"\"\n",
    "    dict_final = {}\n",
    "    size_3d = 13*9*nz\n",
    "    print(\"Now, we get the variables we want\")\n",
    "    for datetime_key in main_dic: #Quiero iterar sobre las keys de 1º nivel\n",
    "        res = []\n",
    "        for var in list_var:  # compruebo que la variable que voy a sacar está en mi lista\n",
    "            if var in main_dic.get(datetime_key).get(\"var_3d\").keys():\n",
    "                 # compruebo que esa variable está en las de 2º nivel\n",
    "                array_3d = main_dic[datetime_key][\"var_3d\"][var][\"data\"]\n",
    "                # Asigno el array del value de 4º nivel a una variable\n",
    "                arr_3d_nz = []\n",
    "                for j in range(0,len(array_3d), size_3d):\n",
    "                    res.extend(array_3d[j: j+size_3d])\n",
    "\n",
    "        for var in list_var:\n",
    "            if var in main_dic.get(datetime_key).get(\"var_2d\").keys():\n",
    "                array_2d = main_dic[datetime_key][\"var_2d\"][var][\"data\"]\n",
    "                res.extend(array_2d)\n",
    "\n",
    "        #for i in range(len(main_dic.keys())):\n",
    "        dict_final.update({datetime_key:res})\n",
    "\n",
    "    return dict_final\n",
    "\n",
    "\n",
    "#names = ['date','data']\n",
    "#formats = ['f8','f8']\n",
    "#dtype = dict(names = names, formats=formats)\n",
    "#array = np.array(list(var_to_test.items()), dtype=dtype)\n",
    "\n",
    "#print(repr(array))\n",
    "\n",
    "\n",
    "\n",
    "def setting_X(dictionary):\n",
    "    meteo_df = pd.DataFrame(dictionary).T\n",
    "    meteo_df.reset_index(level=0, inplace=True)\n",
    "    meteo_df[\"date\"]=pd.to_datetime(meteo_df['index'], format='%d/%m/%Y %H:%M')\n",
    "    meteo_df=meteo_df.sort_values(by='date',ascending=True)\n",
    "    meteo_df=meteo_df.set_index(\"date\").sort_index().loc[:'31/12/2016 00:00']\n",
    "    meteo_df=meteo_df[[x for x in meteo_df.columns if x != 'index']]\n",
    "\n",
    "    return meteo_df\n",
    "\n",
    "\n",
    "def setting_y(csv_file):\n",
    "    power_df = pd.read_csv(csv_file)\n",
    "    power_df['date'] = pd.to_datetime(power_df['date'], format='%d/%m/%Y %H:%M')\n",
    "    power_df = power_df.sort_values(by='date',ascending=True)\n",
    "    power_df=power_df.set_index(\"date\").sort_index().loc[:'31/12/2016 00:00']\n",
    "\n",
    "    return power_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vvel(main_dic, nz=26):\n",
    "    \"\"\"TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n",
    "        EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "        SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
    "        TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n",
    "    \"\"\"\n",
    "    dict_final = {}\n",
    "    res = []\n",
    "    for datetime_key in main_dic.keys(): #Quiero iterar sobre las keys de 1º nivel\n",
    "        if \"Vel100m\" in main_dic.get(datetime_key).get(\"var_2d\").keys():\n",
    "            for j in range(len(main_dic.get(datetime_key).get(\"var_2d\").keys())):\n",
    "                array_2d = main_dic[datetime_key][\"var_2d\"][\"Vel100m\"].get(\"data\")\n",
    "                res.extend(array_2d)\n",
    "\n",
    "        #for i in range(len(main_dic.keys())):\n",
    "        dict_final.update({datetime_key:res})\n",
    "\n",
    "    return dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velo = get_vvel(data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velo = pd.DataFrame(velo)\n",
    "velo.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/slimbook/git-repos/eolo-project/data/.raw/GFS_data\"\n",
    "csv_path = (\"/home/slimbook/git-repos/eolo-project/data/processed/power_data.csv\")\n",
    "list_var = [\"Vel100m\"]\n",
    "\n",
    "print(\"loading raw data and processing it...\")\n",
    "\n",
    "data_dictionary = loading(file_path) # All files and variables loaded as dictionary\n",
    "\n",
    "def to_pickle(input_file):\n",
    "    pickle_out = open('pickledictionary.pickle','wb')\n",
    "    pickle.dump(input_file, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "to_pickle(data_dictionary)\n",
    "#var_to_test = get_var(data_dictionary, list_var, nz=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.functions import loading, get_var, setting_X, setting_y\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "file_path = \"/home/slimbook/git-repos/eolo-project/data/.raw/GFS_data\"\n",
    "csv_path = (\"/home/slimbook/git-repos/eolo-project/data/processed/power_data.csv\")\n",
    "list_var = [\"Vel100m\"]\n",
    "\n",
    "print(\"loading raw data and processing it...\")\n",
    "\n",
    "data_dictionary = loading(file_path) # All files and variables loaded as dictionary\n",
    "var_to_test = get_var(data_dictionary, list_var, nz=5)\n",
    "\n",
    "print(\"Setting X and y data...\")\n",
    "\n",
    "meteo = setting_X(var_to_test)\n",
    "power = setting_y(csv_path)\n",
    "\n",
    "train = pd.concat([power, meteo], axis=1, join=\"inner\")\n",
    "train.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "X = train[[x for x in train.columns if x != 'Production']]\n",
    "y = pd.DataFrame(train[\"Production\"])\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=1000)\n",
    "model.fit(X_train.values, y_train.values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
